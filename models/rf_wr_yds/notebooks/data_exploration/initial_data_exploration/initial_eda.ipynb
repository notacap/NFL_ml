{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NFL Wide Receiver Receiving Yards - Initial Exploratory Data Analysis\n",
    "\n",
    "**Purpose**: Comprehensive statistical analysis of WR receiving yards dataset to inform feature engineering strategy\n",
    "\n",
    "**Date**: 2024-11-24\n",
    "\n",
    "**Dataset**: nfl_wr_receiving_yards_dataset_20251124_184724.parquet\n",
    "\n",
    "**Analysis Phases**:\n",
    "1. Data Understanding & Schema Analysis\n",
    "2. Data Quality Assessment\n",
    "3. Statistical Profiling (Univariate)\n",
    "4. Target Variable Deep Dive\n",
    "5. Correlation Analysis\n",
    "6. Feature Relationship Discovery\n",
    "7. Feature Engineering Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.stats import shapiro, normaltest, skew, kurtosis\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set visualization style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "\n",
    "# Define paths\n",
    "DATA_PATH = Path(r'C:\\Users\\nocap\\Desktop\\code\\NFL_ml\\models\\rf_wr_yds\\data\\processed')\n",
    "OUTPUT_PATH = Path(r'C:\\Users\\nocap\\Desktop\\code\\NFL_ml\\models\\rf_wr_yds\\outputs\\initial_data_exploration')\n",
    "IMAGE_PATH = OUTPUT_PATH / 'images'\n",
    "CSV_PATH = OUTPUT_PATH / 'csv'\n",
    "\n",
    "print(\"Libraries loaded successfully\")\n",
    "print(f\"Data path: {DATA_PATH}\")\n",
    "print(f\"Output path: {OUTPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 1: Data Understanding & Schema Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "dataset_file = DATA_PATH / 'nfl_wr_receiving_yards_dataset_20251124_184724.parquet'\n",
    "df = pd.read_parquet(dataset_file)\n",
    "\n",
    "print(\"Dataset loaded successfully\")\n",
    "print(f\"Shape: {df.shape[0]:,} rows x {df.shape[1]} columns\")\n",
    "print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schema analysis\n",
    "schema_info = pd.DataFrame({\n",
    "    'Column': df.columns,\n",
    "    'Data Type': df.dtypes.values,\n",
    "    'Null Count': df.isnull().sum().values,\n",
    "    'Null Pct': (df.isnull().sum() / len(df) * 100).values,\n",
    "    'Unique Values': df.nunique().values,\n",
    "    'Cardinality': df.nunique().values / len(df),\n",
    "    'Sample Value': [df[col].iloc[0] if len(df) > 0 else None for col in df.columns]\n",
    "})\n",
    "\n",
    "print(\"\\nDataset Schema:\")\n",
    "print(schema_info.to_string(index=False))\n",
    "\n",
    "# Save schema\n",
    "schema_info.to_csv(CSV_PATH / 'dataset_schema.csv', index=False)\n",
    "print(f\"\\nSchema saved to {CSV_PATH / 'dataset_schema.csv'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify column categories\n",
    "id_cols = ['plyr_id', 'plyr_guid', 'season_id', 'week_id', 'game_id', 'adv_plyr_gm_rec_id', 'plyr_rec_id', 'team_id']\n",
    "temporal_cols = ['year', 'week_num']\n",
    "target_col = 'next_week_rec_yds'\n",
    "current_week_target = 'plyr_gm_rec_yds'\n",
    "\n",
    "# Indicator variables (binary flags for imputed nulls)\n",
    "indicator_cols = [col for col in df.columns if col.startswith('plyr_') and col.endswith(('_no_', '_missing_'))]\n",
    "\n",
    "# Game-level features (plyr_gm prefix)\n",
    "game_level_cols = [col for col in df.columns if col.startswith('plyr_gm_rec_') and col != current_week_target and col not in indicator_cols]\n",
    "\n",
    "# Season-level cumulative features (plyr_rec prefix, no _gm_)\n",
    "season_level_cols = [col for col in df.columns if col.startswith('plyr_rec_') and 'plyr_gm_rec' not in col and col not in indicator_cols and col != 'plyr_rec_id']\n",
    "\n",
    "# Other features\n",
    "other_cols = [col for col in df.columns if col not in id_cols + temporal_cols + [target_col, current_week_target] + indicator_cols + game_level_cols + season_level_cols]\n",
    "\n",
    "print(f\"\\nColumn Categories:\")\n",
    "print(f\"  ID Columns: {len([c for c in id_cols if c in df.columns])}\")\n",
    "print(f\"  Temporal Columns: {len([c for c in temporal_cols if c in df.columns])}\")\n",
    "print(f\"  Target Column: {target_col}\")\n",
    "print(f\"  Current Week Target: {current_week_target}\")\n",
    "print(f\"  Indicator Variables: {len(indicator_cols)}\")\n",
    "print(f\"  Game-level Features: {len(game_level_cols)}\")\n",
    "print(f\"  Season-level Features: {len(season_level_cols)}\")\n",
    "print(f\"  Other Features: {len(other_cols)}\")\n",
    "\n",
    "print(f\"\\nGame-level features: {game_level_cols}\")\n",
    "print(f\"\\nSeason-level features: {season_level_cols}\")\n",
    "print(f\"\\nIndicator variables: {indicator_cols}\")\n",
    "print(f\"\\nOther features: {other_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic dataset statistics\n",
    "print(\"\\nDataset Overview:\")\n",
    "print(f\"  Date range: {df['year'].min()} to {df['year'].max()}\")\n",
    "print(f\"  Week range: Week {df['week_num'].min()} to Week {df['week_num'].max()}\")\n",
    "print(f\"  Unique players: {df['plyr_id'].nunique():,}\")\n",
    "print(f\"  Unique seasons: {df['season_id'].nunique()}\")\n",
    "print(f\"  Unique teams: {df['team_id'].nunique()}\")\n",
    "print(f\"  Average samples per player: {len(df) / df['plyr_id'].nunique():.1f}\")\n",
    "\n",
    "# Temporal distribution\n",
    "print(\"\\nSamples by Season:\")\n",
    "print(df.groupby('year').size().sort_index())\n",
    "\n",
    "print(\"\\nSamples by Week:\")\n",
    "print(df.groupby('week_num').size().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 2: Data Quality Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing value analysis\n",
    "missing_analysis = pd.DataFrame({\n",
    "    'Column': df.columns,\n",
    "    'Missing Count': df.isnull().sum().values,\n",
    "    'Missing Percentage': (df.isnull().sum() / len(df) * 100).values\n",
    "}).sort_values('Missing Percentage', ascending=False)\n",
    "\n",
    "print(\"Missing Values Analysis:\")\n",
    "print(missing_analysis[missing_analysis['Missing Percentage'] > 0].to_string(index=False))\n",
    "\n",
    "if missing_analysis['Missing Percentage'].sum() == 0:\n",
    "    print(\"\\nEXCELLENT: No missing values detected in dataset\")\n",
    "    print(\"This indicates proper null handling with indicator variables\")\n",
    "\n",
    "# Save missing value analysis\n",
    "missing_analysis.to_csv(CSV_PATH / 'missing_values_analysis.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier detection using IQR method\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "numeric_cols = [col for col in numeric_cols if col not in id_cols + temporal_cols + indicator_cols]\n",
    "\n",
    "outlier_summary = []\n",
    "\n",
    "for col in numeric_cols:\n",
    "    Q1 = df[col].quantile(0.25)\n",
    "    Q3 = df[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 3 * IQR\n",
    "    upper_bound = Q3 + 3 * IQR\n",
    "    \n",
    "    outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]\n",
    "    outlier_pct = len(outliers) / len(df) * 100\n",
    "    \n",
    "    outlier_summary.append({\n",
    "        'Column': col,\n",
    "        'Outlier Count': len(outliers),\n",
    "        'Outlier Pct': outlier_pct,\n",
    "        'Lower Bound': lower_bound,\n",
    "        'Upper Bound': upper_bound,\n",
    "        'Min Value': df[col].min(),\n",
    "        'Max Value': df[col].max()\n",
    "    })\n",
    "\n",
    "outlier_df = pd.DataFrame(outlier_summary).sort_values('Outlier Pct', ascending=False)\n",
    "print(\"\\nOutlier Analysis (Top 20 by percentage):\")\n",
    "print(outlier_df.head(20).to_string(index=False))\n",
    "\n",
    "# Save outlier analysis\n",
    "outlier_df.to_csv(CSV_PATH / 'outlier_analysis.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indicator variable patterns\n",
    "if indicator_cols:\n",
    "    print(\"\\nIndicator Variable Activation Rates:\")\n",
    "    indicator_stats = pd.DataFrame({\n",
    "        'Indicator': indicator_cols,\n",
    "        'Activation Count': [df[col].sum() for col in indicator_cols],\n",
    "        'Activation Rate': [df[col].mean() * 100 for col in indicator_cols]\n",
    "    }).sort_values('Activation Rate', ascending=False)\n",
    "    \n",
    "    print(indicator_stats.to_string(index=False))\n",
    "    \n",
    "    # Save indicator analysis\n",
    "    indicator_stats.to_csv(CSV_PATH / 'indicator_variable_analysis.csv', index=False)\n",
    "    \n",
    "    # Visualize top indicators\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    top_indicators = indicator_stats.head(15)\n",
    "    ax.barh(range(len(top_indicators)), top_indicators['Activation Rate'])\n",
    "    ax.set_yticks(range(len(top_indicators)))\n",
    "    ax.set_yticklabels(top_indicators['Indicator'])\n",
    "    ax.set_xlabel('Activation Rate (%)')\n",
    "    ax.set_title('Top 15 Indicator Variables by Activation Rate')\n",
    "    ax.invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(IMAGE_PATH / 'indicator_activation_rates.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"\\nIndicator chart saved to {IMAGE_PATH / 'indicator_activation_rates.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 3: Statistical Profiling (Univariate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive univariate statistics for numeric features\n",
    "numeric_features = [col for col in numeric_cols if col != target_col and col != current_week_target]\n",
    "\n",
    "univariate_stats = []\n",
    "\n",
    "for col in numeric_features:\n",
    "    data = df[col].values\n",
    "    \n",
    "    stats_dict = {\n",
    "        'Feature': col,\n",
    "        'Count': len(data),\n",
    "        'Mean': np.mean(data),\n",
    "        'Median': np.median(data),\n",
    "        'Std': np.std(data),\n",
    "        'Min': np.min(data),\n",
    "        'Q1': np.percentile(data, 25),\n",
    "        'Q3': np.percentile(data, 75),\n",
    "        'Max': np.max(data),\n",
    "        'Skewness': skew(data),\n",
    "        'Kurtosis': kurtosis(data),\n",
    "        'Coefficient of Variation': np.std(data) / np.mean(data) if np.mean(data) != 0 else 0,\n",
    "        'Range': np.max(data) - np.min(data)\n",
    "    }\n",
    "    \n",
    "    univariate_stats.append(stats_dict)\n",
    "\n",
    "univariate_df = pd.DataFrame(univariate_stats)\n",
    "print(\"\\nUnivariate Statistics Summary (First 20 features):\")\n",
    "print(univariate_df.head(20).to_string(index=False))\n",
    "\n",
    "# Save univariate stats\n",
    "univariate_df.to_csv(CSV_PATH / 'univariate_statistics.csv', index=False)\n",
    "print(f\"\\nFull univariate statistics saved to {CSV_PATH / 'univariate_statistics.csv'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution characteristics\n",
    "print(\"\\nDistribution Characteristics:\")\n",
    "print(f\"\\nHighly Skewed Features (|skewness| > 2):\")\n",
    "high_skew = univariate_df[abs(univariate_df['Skewness']) > 2].sort_values('Skewness', ascending=False)\n",
    "print(high_skew[['Feature', 'Skewness', 'Mean', 'Median']].to_string(index=False))\n",
    "\n",
    "print(f\"\\nHigh Kurtosis Features (kurtosis > 5):\")\n",
    "high_kurtosis = univariate_df[univariate_df['Kurtosis'] > 5].sort_values('Kurtosis', ascending=False)\n",
    "print(high_kurtosis[['Feature', 'Kurtosis', 'Min', 'Max']].to_string(index=False))\n",
    "\n",
    "print(f\"\\nHigh Variance Features (CV > 1.0):\")\n",
    "high_var = univariate_df[univariate_df['Coefficient of Variation'] > 1.0].sort_values('Coefficient of Variation', ascending=False)\n",
    "print(high_var[['Feature', 'Coefficient of Variation', 'Mean', 'Std']].head(15).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 4: Target Variable Deep Dive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target variable comprehensive analysis\n",
    "target = df[target_col]\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TARGET VARIABLE ANALYSIS: next_week_rec_yds\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Basic statistics\n",
    "print(f\"\\nBasic Statistics:\")\n",
    "print(f\"  Count: {len(target):,}\")\n",
    "print(f\"  Mean: {target.mean():.2f} yards\")\n",
    "print(f\"  Median: {target.median():.2f} yards\")\n",
    "print(f\"  Std Dev: {target.std():.2f} yards\")\n",
    "print(f\"  Min: {target.min():.0f} yards\")\n",
    "print(f\"  Max: {target.max():.0f} yards\")\n",
    "print(f\"  25th Percentile: {target.quantile(0.25):.2f} yards\")\n",
    "print(f\"  75th Percentile: {target.quantile(0.75):.2f} yards\")\n",
    "print(f\"  IQR: {target.quantile(0.75) - target.quantile(0.25):.2f} yards\")\n",
    "\n",
    "# Distribution characteristics\n",
    "print(f\"\\nDistribution Characteristics:\")\n",
    "print(f\"  Skewness: {skew(target):.3f} (Right-skewed)\" if skew(target) > 0 else f\"  Skewness: {skew(target):.3f} (Left-skewed)\")\n",
    "print(f\"  Kurtosis: {kurtosis(target):.3f}\")\n",
    "print(f\"  Coefficient of Variation: {target.std() / target.mean():.3f}\")\n",
    "\n",
    "# Zero yards analysis\n",
    "zero_yards = (target == 0).sum()\n",
    "print(f\"\\nZero Yards Games:\")\n",
    "print(f\"  Count: {zero_yards:,}\")\n",
    "print(f\"  Percentage: {zero_yards / len(target) * 100:.2f}%\")\n",
    "\n",
    "# Categorization\n",
    "print(f\"\\nTarget Value Ranges:\")\n",
    "print(f\"  0 yards: {(target == 0).sum():,} ({(target == 0).sum() / len(target) * 100:.1f}%)\")\n",
    "print(f\"  1-50 yards: {((target > 0) & (target <= 50)).sum():,} ({((target > 0) & (target <= 50)).sum() / len(target) * 100:.1f}%)\")\n",
    "print(f\"  51-100 yards: {((target > 50) & (target <= 100)).sum():,} ({((target > 50) & (target <= 100)).sum() / len(target) * 100:.1f}%)\")\n",
    "print(f\"  101-150 yards: {((target > 100) & (target <= 150)).sum():,} ({((target > 100) & (target <= 150)).sum() / len(target) * 100:.1f}%)\")\n",
    "print(f\"  150+ yards: {(target > 150).sum():,} ({(target > 150).sum() / len(target) * 100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target distribution visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Histogram\n",
    "axes[0, 0].hist(target, bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[0, 0].axvline(target.mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {target.mean():.1f}')\n",
    "axes[0, 0].axvline(target.median(), color='green', linestyle='--', linewidth=2, label=f'Median: {target.median():.1f}')\n",
    "axes[0, 0].set_xlabel('Receiving Yards')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].set_title('Distribution of Next Week Receiving Yards')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# Box plot\n",
    "axes[0, 1].boxplot(target, vert=True)\n",
    "axes[0, 1].set_ylabel('Receiving Yards')\n",
    "axes[0, 1].set_title('Box Plot: Next Week Receiving Yards')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Q-Q plot\n",
    "stats.probplot(target, dist=\"norm\", plot=axes[1, 0])\n",
    "axes[1, 0].set_title('Q-Q Plot: Normality Assessment')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Log-transformed histogram (for skewness assessment)\n",
    "log_target = np.log1p(target)  # log1p handles zeros\n",
    "axes[1, 1].hist(log_target, bins=50, edgecolor='black', alpha=0.7, color='orange')\n",
    "axes[1, 1].axvline(log_target.mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {log_target.mean():.2f}')\n",
    "axes[1, 1].set_xlabel('Log(1 + Receiving Yards)')\n",
    "axes[1, 1].set_ylabel('Frequency')\n",
    "axes[1, 1].set_title('Log-Transformed Distribution')\n",
    "axes[1, 1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(IMAGE_PATH / 'target_distribution_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(f\"\\nTarget distribution charts saved to {IMAGE_PATH / 'target_distribution_analysis.png'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Week-over-week volatility analysis\n",
    "print(\"\\nWeek-over-Week Volatility Analysis:\")\n",
    "\n",
    "# Calculate week-over-week changes for each player\n",
    "df_sorted = df.sort_values(['plyr_id', 'season_id', 'week_num'])\n",
    "df_sorted['wow_change'] = df_sorted.groupby(['plyr_id', 'season_id'])[target_col].diff()\n",
    "df_sorted['wow_pct_change'] = df_sorted.groupby(['plyr_id', 'season_id'])[target_col].pct_change()\n",
    "\n",
    "# Remove infinites from percentage changes\n",
    "wow_pct_clean = df_sorted['wow_pct_change'].replace([np.inf, -np.inf], np.nan).dropna()\n",
    "\n",
    "print(f\"  Mean absolute week-over-week change: {df_sorted['wow_change'].abs().mean():.2f} yards\")\n",
    "print(f\"  Median absolute week-over-week change: {df_sorted['wow_change'].abs().median():.2f} yards\")\n",
    "print(f\"  Std dev of week-over-week changes: {df_sorted['wow_change'].std():.2f} yards\")\n",
    "print(f\"  Mean absolute percentage change: {wow_pct_clean.abs().mean() * 100:.2f}%\")\n",
    "\n",
    "# Identify high volatility players\n",
    "player_volatility = df_sorted.groupby('plyr_id')['wow_change'].apply(lambda x: x.abs().mean()).sort_values(ascending=False)\n",
    "print(f\"\\nTop 10 Most Volatile Players (by mean absolute change):\")\n",
    "print(player_volatility.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target by week analysis\n",
    "target_by_week = df.groupby('week_num')[target_col].agg(['mean', 'median', 'std', 'count'])\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 10))\n",
    "\n",
    "# Mean/median by week\n",
    "axes[0].plot(target_by_week.index, target_by_week['mean'], marker='o', label='Mean', linewidth=2)\n",
    "axes[0].plot(target_by_week.index, target_by_week['median'], marker='s', label='Median', linewidth=2)\n",
    "axes[0].fill_between(target_by_week.index, \n",
    "                       target_by_week['mean'] - target_by_week['std'],\n",
    "                       target_by_week['mean'] + target_by_week['std'],\n",
    "                       alpha=0.2)\n",
    "axes[0].set_xlabel('Week Number')\n",
    "axes[0].set_ylabel('Receiving Yards')\n",
    "axes[0].set_title('Target Variable by Week (with Std Dev Band)')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Sample count by week\n",
    "axes[1].bar(target_by_week.index, target_by_week['count'], alpha=0.7)\n",
    "axes[1].set_xlabel('Week Number')\n",
    "axes[1].set_ylabel('Sample Count')\n",
    "axes[1].set_title('Number of Samples by Week')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(IMAGE_PATH / 'target_by_week_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(f\"\\nTarget by week chart saved to {IMAGE_PATH / 'target_by_week_analysis.png'}\")\n",
    "\n",
    "# Save weekly stats\n",
    "target_by_week.to_csv(CSV_PATH / 'target_by_week_statistics.csv')\n",
    "print(f\"Weekly statistics saved to {CSV_PATH / 'target_by_week_statistics.csv'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 5: Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlations with target\n",
    "# Exclude ID columns, temporal columns, current week target, and indicator variables\n",
    "feature_cols = [col for col in df.columns if col not in id_cols + temporal_cols + [target_col, current_week_target] + indicator_cols]\n",
    "feature_cols_numeric = [col for col in feature_cols if df[col].dtype in ['int64', 'float64']]\n",
    "\n",
    "# Pearson correlation\n",
    "pearson_corr = df[feature_cols_numeric + [target_col]].corr()[target_col].drop(target_col).sort_values(ascending=False)\n",
    "\n",
    "# Spearman correlation (rank-based, handles non-linear monotonic relationships)\n",
    "spearman_corr = df[feature_cols_numeric + [target_col]].corr(method='spearman')[target_col].drop(target_col).sort_values(ascending=False)\n",
    "\n",
    "# Combine correlations\n",
    "correlation_df = pd.DataFrame({\n",
    "    'Feature': pearson_corr.index,\n",
    "    'Pearson_Correlation': pearson_corr.values,\n",
    "    'Spearman_Correlation': spearman_corr[pearson_corr.index].values,\n",
    "    'Abs_Pearson': abs(pearson_corr.values),\n",
    "    'Abs_Spearman': abs(spearman_corr[pearson_corr.index].values)\n",
    "}).sort_values('Abs_Pearson', ascending=False)\n",
    "\n",
    "print(\"\\nTop 25 Features by Absolute Pearson Correlation with Target:\")\n",
    "print(correlation_df.head(25).to_string(index=False))\n",
    "\n",
    "# Save correlation analysis\n",
    "correlation_df.to_csv(CSV_PATH / 'feature_target_correlations.csv', index=False)\n",
    "print(f\"\\nCorrelation analysis saved to {CSV_PATH / 'feature_target_correlations.csv'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize top correlations\n",
    "top_features = correlation_df.head(20)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 10))\n",
    "\n",
    "# Pearson correlations\n",
    "axes[0].barh(range(len(top_features)), top_features['Pearson_Correlation'])\n",
    "axes[0].set_yticks(range(len(top_features)))\n",
    "axes[0].set_yticklabels(top_features['Feature'])\n",
    "axes[0].set_xlabel('Pearson Correlation')\n",
    "axes[0].set_title('Top 20 Features: Pearson Correlation with Target')\n",
    "axes[0].axvline(x=0, color='black', linestyle='--', linewidth=0.8)\n",
    "axes[0].invert_yaxis()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Spearman correlations\n",
    "axes[1].barh(range(len(top_features)), top_features['Spearman_Correlation'], color='orange')\n",
    "axes[1].set_yticks(range(len(top_features)))\n",
    "axes[1].set_yticklabels(top_features['Feature'])\n",
    "axes[1].set_xlabel('Spearman Correlation')\n",
    "axes[1].set_title('Top 20 Features: Spearman Correlation with Target')\n",
    "axes[1].axvline(x=0, color='black', linestyle='--', linewidth=0.8)\n",
    "axes[1].invert_yaxis()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(IMAGE_PATH / 'correlation_top_features.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(f\"\\nCorrelation chart saved to {IMAGE_PATH / 'correlation_top_features.png'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multicollinearity analysis - correlation matrix of top features\n",
    "top_20_features = correlation_df.head(20)['Feature'].tolist()\n",
    "correlation_matrix = df[top_20_features].corr()\n",
    "\n",
    "# Find highly correlated pairs\n",
    "high_corr_pairs = []\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i+1, len(correlation_matrix.columns)):\n",
    "        if abs(correlation_matrix.iloc[i, j]) > 0.8:\n",
    "            high_corr_pairs.append({\n",
    "                'Feature_1': correlation_matrix.columns[i],\n",
    "                'Feature_2': correlation_matrix.columns[j],\n",
    "                'Correlation': correlation_matrix.iloc[i, j]\n",
    "            })\n",
    "\n",
    "if high_corr_pairs:\n",
    "    high_corr_df = pd.DataFrame(high_corr_pairs).sort_values('Correlation', ascending=False, key=abs)\n",
    "    print(\"\\nHighly Correlated Feature Pairs (|r| > 0.8):\")\n",
    "    print(high_corr_df.to_string(index=False))\n",
    "    high_corr_df.to_csv(CSV_PATH / 'multicollinearity_pairs.csv', index=False)\n",
    "else:\n",
    "    print(\"\\nNo highly correlated pairs found (|r| > 0.8) among top 20 features\")\n",
    "\n",
    "# Heatmap of top features\n",
    "plt.figure(figsize=(14, 12))\n",
    "sns.heatmap(correlation_matrix, annot=False, cmap='coolwarm', center=0, \n",
    "            cbar_kws={'label': 'Correlation'}, vmin=-1, vmax=1)\n",
    "plt.title('Correlation Matrix: Top 20 Predictive Features')\n",
    "plt.tight_layout()\n",
    "plt.savefig(IMAGE_PATH / 'correlation_matrix_heatmap.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(f\"\\nCorrelation matrix heatmap saved to {IMAGE_PATH / 'correlation_matrix_heatmap.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 6: Feature Relationship Discovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plots for top 6 features\n",
    "top_6_features = correlation_df.head(6)['Feature'].tolist()\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, feature in enumerate(top_6_features):\n",
    "    # Sample for visualization (to avoid overplotting)\n",
    "    sample_size = min(1000, len(df))\n",
    "    sample_df = df.sample(n=sample_size, random_state=42)\n",
    "    \n",
    "    axes[idx].scatter(sample_df[feature], sample_df[target_col], alpha=0.5, s=10)\n",
    "    \n",
    "    # Add regression line\n",
    "    z = np.polyfit(sample_df[feature], sample_df[target_col], 1)\n",
    "    p = np.poly1d(z)\n",
    "    x_line = np.linspace(sample_df[feature].min(), sample_df[feature].max(), 100)\n",
    "    axes[idx].plot(x_line, p(x_line), \"r--\", alpha=0.8, linewidth=2)\n",
    "    \n",
    "    corr_val = correlation_df[correlation_df['Feature'] == feature]['Pearson_Correlation'].values[0]\n",
    "    axes[idx].set_xlabel(feature)\n",
    "    axes[idx].set_ylabel(target_col)\n",
    "    axes[idx].set_title(f'{feature}\\n(r = {corr_val:.3f})')\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(IMAGE_PATH / 'scatter_top_features_vs_target.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(f\"\\nScatter plots saved to {IMAGE_PATH / 'scatter_top_features_vs_target.png'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify potential interaction candidates\n",
    "# Look for feature pairs with moderate correlation to target and low correlation to each other\n",
    "interaction_candidates = []\n",
    "\n",
    "top_30_features = correlation_df.head(30)['Feature'].tolist()\n",
    "\n",
    "for i in range(len(top_30_features)):\n",
    "    for j in range(i+1, len(top_30_features)):\n",
    "        feat1 = top_30_features[i]\n",
    "        feat2 = top_30_features[j]\n",
    "        \n",
    "        # Correlation between features\n",
    "        feat_corr = df[[feat1, feat2]].corr().iloc[0, 1]\n",
    "        \n",
    "        # Correlations with target\n",
    "        feat1_target_corr = correlation_df[correlation_df['Feature'] == feat1]['Abs_Pearson'].values[0]\n",
    "        feat2_target_corr = correlation_df[correlation_df['Feature'] == feat2]['Abs_Pearson'].values[0]\n",
    "        \n",
    "        # Good interaction candidates: both correlated with target, but not too correlated with each other\n",
    "        if (feat1_target_corr > 0.1 and feat2_target_corr > 0.1 and abs(feat_corr) < 0.7):\n",
    "            interaction_candidates.append({\n",
    "                'Feature_1': feat1,\n",
    "                'Feature_2': feat2,\n",
    "                'Feature_Correlation': feat_corr,\n",
    "                'Feat1_Target_Corr': feat1_target_corr,\n",
    "                'Feat2_Target_Corr': feat2_target_corr,\n",
    "                'Interaction_Score': feat1_target_corr * feat2_target_corr * (1 - abs(feat_corr))\n",
    "            })\n",
    "\n",
    "interaction_df = pd.DataFrame(interaction_candidates).sort_values('Interaction_Score', ascending=False)\n",
    "print(\"\\nTop 20 Potential Interaction Feature Pairs:\")\n",
    "print(interaction_df.head(20).to_string(index=False))\n",
    "\n",
    "# Save interaction candidates\n",
    "interaction_df.to_csv(CSV_PATH / 'interaction_candidates.csv', index=False)\n",
    "print(f\"\\nInteraction candidates saved to {CSV_PATH / 'interaction_candidates.csv'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify transformation opportunities\n",
    "# Features with high skewness may benefit from log transformation\n",
    "transformation_opportunities = univariate_df[abs(univariate_df['Skewness']) > 1.5].copy()\n",
    "transformation_opportunities = transformation_opportunities.merge(\n",
    "    correlation_df[['Feature', 'Abs_Pearson']], \n",
    "    on='Feature', \n",
    "    how='left'\n",
    ")\n",
    "transformation_opportunities = transformation_opportunities.sort_values('Abs_Pearson', ascending=False)\n",
    "\n",
    "print(\"\\nFeatures with High Skewness (Transformation Candidates):\")\n",
    "print(transformation_opportunities[['Feature', 'Skewness', 'Abs_Pearson', 'Mean', 'Std']].head(20).to_string(index=False))\n",
    "\n",
    "# Save transformation opportunities\n",
    "transformation_opportunities.to_csv(CSV_PATH / 'transformation_opportunities.csv', index=False)\n",
    "print(f\"\\nTransformation opportunities saved to {CSV_PATH / 'transformation_opportunities.csv'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 7: Feature Engineering Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate feature recommendations\n",
    "basic_features = []\n",
    "complex_features = []\n",
    "\n",
    "# BASIC FEATURES (for pipeline validation)\n",
    "\n",
    "# 1. Rolling averages of key stats\n",
    "top_stats = correlation_df.head(10)['Feature'].tolist()\n",
    "for stat in top_stats:\n",
    "    if 'plyr_gm_rec' in stat or 'plyr_rec' in stat:\n",
    "        for window in [3, 5]:\n",
    "            basic_features.append({\n",
    "                'feature_name': f'{stat}_rolling_{window}gm_avg',\n",
    "                'description': f'Rolling {window}-game average of {stat}',\n",
    "                'category': 'basic',\n",
    "                'priority': 5 if window == 3 else 4,\n",
    "                'rationale': f'Recent performance trend. Base stat correlation: {correlation_df[correlation_df[\"Feature\"] == stat][\"Pearson_Correlation\"].values[0]:.3f}',\n",
    "                'implementation_notes': f'Group by plyr_id, season_id and calculate rolling mean with window={window}',\n",
    "                'estimated_correlation': abs(correlation_df[correlation_df['Feature'] == stat]['Pearson_Correlation'].values[0]) * 0.9\n",
    "            })\n",
    "\n",
    "# 2. Season-to-date per-game averages\n",
    "basic_features.append({\n",
    "    'feature_name': 'season_targets_per_game',\n",
    "    'description': 'Season-to-date targets per game',\n",
    "    'category': 'basic',\n",
    "    'priority': 5,\n",
    "    'rationale': 'Normalizes target volume by games played, handles varying sample sizes',\n",
    "    'implementation_notes': 'plyr_rec_tgt / plyr_rec_gm',\n",
    "    'estimated_correlation': 0.35\n",
    "})\n",
    "\n",
    "basic_features.append({\n",
    "    'feature_name': 'season_yards_per_game',\n",
    "    'description': 'Season-to-date receiving yards per game',\n",
    "    'category': 'basic',\n",
    "    'priority': 5,\n",
    "    'rationale': 'Normalizes cumulative yards by games played',\n",
    "    'implementation_notes': 'plyr_rec_yds / plyr_rec_gm',\n",
    "    'estimated_correlation': 0.40\n",
    "})\n",
    "\n",
    "# 3. Efficiency metrics\n",
    "basic_features.append({\n",
    "    'feature_name': 'catch_rate',\n",
    "    'description': 'Receptions per target (catch rate)',\n",
    "    'category': 'basic',\n",
    "    'priority': 4,\n",
    "    'rationale': 'Measures efficiency and QB-WR connection quality',\n",
    "    'implementation_notes': 'plyr_rec / plyr_rec_tgt (already exists as plyr_rec_catch_pct)',\n",
    "    'estimated_correlation': 0.15\n",
    "})\n",
    "\n",
    "basic_features.append({\n",
    "    'feature_name': 'yards_per_reception',\n",
    "    'description': 'Average yards per reception',\n",
    "    'category': 'basic',\n",
    "    'priority': 4,\n",
    "    'rationale': 'Indicates big-play ability and route depth',\n",
    "    'implementation_notes': 'plyr_rec_yds / plyr_rec',\n",
    "    'estimated_correlation': 0.20\n",
    "})\n",
    "\n",
    "basic_features.append({\n",
    "    'feature_name': 'yards_per_target',\n",
    "    'description': 'Average yards per target',\n",
    "    'category': 'basic',\n",
    "    'priority': 4,\n",
    "    'rationale': 'Overall efficiency metric combining catch rate and YPR',\n",
    "    'implementation_notes': 'plyr_rec_yds / plyr_rec_tgt (already exists as plyr_rec_yds_tgt)',\n",
    "    'estimated_correlation': 0.25\n",
    "})\n",
    "\n",
    "# 4. Momentum indicators\n",
    "basic_features.append({\n",
    "    'feature_name': 'yds_trend_last_3_games',\n",
    "    'description': 'Linear trend of yards over last 3 games',\n",
    "    'category': 'basic',\n",
    "    'priority': 3,\n",
    "    'rationale': 'Captures improving/declining performance trajectory',\n",
    "    'implementation_notes': 'Calculate slope of linear regression on last 3 games yards',\n",
    "    'estimated_correlation': 0.18\n",
    "})\n",
    "\n",
    "basic_features.append({\n",
    "    'feature_name': 'target_share_trend',\n",
    "    'description': 'Change in target share over last 3 games',\n",
    "    'category': 'basic',\n",
    "    'priority': 3,\n",
    "    'rationale': 'Identifies increased/decreased role in offense',\n",
    "    'implementation_notes': 'Compare current 3-game avg targets to previous 3-game avg',\n",
    "    'estimated_correlation': 0.15\n",
    "})\n",
    "\n",
    "# 5. Volatility measures\n",
    "basic_features.append({\n",
    "    'feature_name': 'yards_std_last_5_games',\n",
    "    'description': 'Standard deviation of yards over last 5 games',\n",
    "    'category': 'basic',\n",
    "    'priority': 2,\n",
    "    'rationale': 'Captures consistency/boom-bust tendency',\n",
    "    'implementation_notes': 'Rolling std with window=5 on plyr_gm_rec_yds',\n",
    "    'estimated_correlation': 0.10\n",
    "})\n",
    "\n",
    "# COMPLEX FEATURES (post-validation)\n",
    "\n",
    "# 1. Opponent adjustments\n",
    "complex_features.append({\n",
    "    'feature_name': 'opponent_pass_defense_rank',\n",
    "    'description': 'Opponent pass defense ranking (yards allowed)',\n",
    "    'category': 'complex',\n",
    "    'priority': 5,\n",
    "    'rationale': 'Matchup difficulty - weak pass defenses allow more yards',\n",
    "    'implementation_notes': 'Requires team defense tables join and ranking calculation',\n",
    "    'estimated_correlation': 0.25\n",
    "})\n",
    "\n",
    "complex_features.append({\n",
    "    'feature_name': 'opponent_wr_yards_allowed_avg',\n",
    "    'description': 'Average WR yards allowed by opponent defense',\n",
    "    'category': 'complex',\n",
    "    'priority': 5,\n",
    "    'rationale': 'Direct matchup metric for WR production',\n",
    "    'implementation_notes': 'Requires tm_def_vs_wr table join',\n",
    "    'estimated_correlation': 0.30\n",
    "})\n",
    "\n",
    "# 2. Game script indicators\n",
    "complex_features.append({\n",
    "    'feature_name': 'team_implied_total',\n",
    "    'description': 'Team implied point total from Vegas lines',\n",
    "    'category': 'complex',\n",
    "    'priority': 4,\n",
    "    'rationale': 'High-scoring games favor passing volume',\n",
    "    'implementation_notes': 'Requires external betting data integration',\n",
    "    'estimated_correlation': 0.20\n",
    "})\n",
    "\n",
    "complex_features.append({\n",
    "    'feature_name': 'team_pass_rate_over_expected',\n",
    "    'description': 'Team pass rate vs league average in similar game scripts',\n",
    "    'category': 'complex',\n",
    "    'priority': 3,\n",
    "    'rationale': 'Identifies pass-heavy offenses that create more WR opportunities',\n",
    "    'implementation_notes': 'Requires play-by-play analysis and game script modeling',\n",
    "    'estimated_correlation': 0.22\n",
    "})\n",
    "\n",
    "# 3. Route/target quality\n",
    "complex_features.append({\n",
    "    'feature_name': 'air_yards_share',\n",
    "    'description': 'Player share of team air yards',\n",
    "    'category': 'complex',\n",
    "    'priority': 5,\n",
    "    'rationale': 'Measures downfield target opportunity',\n",
    "    'implementation_notes': 'Player ADOT * targets / Team total air yards',\n",
    "    'estimated_correlation': 0.35\n",
    "})\n",
    "\n",
    "complex_features.append({\n",
    "    'feature_name': 'route_participation_rate',\n",
    "    'description': 'Routes run / team pass plays',\n",
    "    'category': 'complex',\n",
    "    'priority': 4,\n",
    "    'rationale': 'More routes = more target opportunities',\n",
    "    'implementation_notes': 'Requires route tracking data (PFF/Next Gen Stats)',\n",
    "    'estimated_correlation': 0.28\n",
    "})\n",
    "\n",
    "# 4. Home/away splits\n",
    "complex_features.append({\n",
    "    'feature_name': 'home_away_split_factor',\n",
    "    'description': 'Player performance difference home vs away',\n",
    "    'category': 'complex',\n",
    "    'priority': 2,\n",
    "    'rationale': 'Some players perform better in specific environments',\n",
    "    'implementation_notes': 'Requires game location data and historical split calculation',\n",
    "    'estimated_correlation': 0.12\n",
    "})\n",
    "\n",
    "# 5. Weather adjustments\n",
    "complex_features.append({\n",
    "    'feature_name': 'weather_severity_score',\n",
    "    'description': 'Composite score of wind, precipitation, temperature',\n",
    "    'category': 'complex',\n",
    "    'priority': 3,\n",
    "    'rationale': 'Severe weather reduces passing efficiency',\n",
    "    'implementation_notes': 'Requires game_weather table join and score calculation',\n",
    "    'estimated_correlation': 0.15\n",
    "})\n",
    "\n",
    "# 6. QB quality metrics\n",
    "complex_features.append({\n",
    "    'feature_name': 'qb_recent_performance',\n",
    "    'description': 'QB passing yards avg last 3 games',\n",
    "    'category': 'complex',\n",
    "    'priority': 4,\n",
    "    'rationale': 'Hot QB lifts all receivers',\n",
    "    'implementation_notes': 'Requires player_game_passing table join',\n",
    "    'estimated_correlation': 0.25\n",
    "})\n",
    "\n",
    "# Combine all recommendations\n",
    "all_recommendations = basic_features + complex_features\n",
    "recommendations_df = pd.DataFrame(all_recommendations)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FEATURE ENGINEERING RECOMMENDATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nBASIC FEATURES (for immediate implementation):\")\n",
    "basic_df = recommendations_df[recommendations_df['category'] == 'basic'].sort_values('priority', ascending=False)\n",
    "print(basic_df[['feature_name', 'priority', 'estimated_correlation', 'description']].to_string(index=False))\n",
    "\n",
    "print(\"\\nCOMPLEX FEATURES (for post-validation):\")\n",
    "complex_df = recommendations_df[recommendations_df['category'] == 'complex'].sort_values('priority', ascending=False)\n",
    "print(complex_df[['feature_name', 'priority', 'estimated_correlation', 'description']].to_string(index=False))\n",
    "\n",
    "# Save recommendations\n",
    "recommendations_df.to_csv(CSV_PATH / 'feature_recommendations.csv', index=False)\n",
    "print(f\"\\n\\nFeature recommendations saved to {CSV_PATH / 'feature_recommendations.csv'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary & Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EXPLORATORY DATA ANALYSIS - SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n1. DATASET OVERVIEW\")\n",
    "print(f\"   - Total samples: {len(df):,}\")\n",
    "print(f\"   - Features: {len([c for c in df.columns if c not in id_cols + temporal_cols + [target_col]])}\")\n",
    "print(f\"   - Unique players: {df['plyr_id'].nunique()}\")\n",
    "print(f\"   - Date range: {df['year'].min()}-{df['year'].max()}, Weeks {df['week_num'].min()}-{df['week_num'].max()}\")\n",
    "\n",
    "print(\"\\n2. DATA QUALITY\")\n",
    "print(f\"   - Missing values: {df.isnull().sum().sum()} (0%)\")\n",
    "print(f\"   - Indicator variables: {len(indicator_cols)} tracking imputed nulls\")\n",
    "print(f\"   - Data integrity: EXCELLENT\")\n",
    "\n",
    "print(\"\\n3. TARGET VARIABLE (next_week_rec_yds)\")\n",
    "print(f\"   - Mean: {target.mean():.1f} yards\")\n",
    "print(f\"   - Median: {target.median():.1f} yards\")\n",
    "print(f\"   - Std Dev: {target.std():.1f} yards\")\n",
    "print(f\"   - Skewness: {skew(target):.2f} (right-skewed)\")\n",
    "print(f\"   - Zero yards games: {(target == 0).sum()} ({(target == 0).sum() / len(target) * 100:.1f}%)\")\n",
    "\n",
    "print(\"\\n4. TOP 10 PREDICTIVE FEATURES\")\n",
    "top_10 = correlation_df.head(10)\n",
    "for idx, row in top_10.iterrows():\n",
    "    print(f\"   {idx+1}. {row['Feature']}: r={row['Pearson_Correlation']:.3f}\")\n",
    "\n",
    "print(\"\\n5. FEATURE RECOMMENDATIONS\")\n",
    "print(f\"   - Basic features recommended: {len(basic_features)}\")\n",
    "print(f\"   - Complex features recommended: {len(complex_features)}\")\n",
    "print(f\"   - High-priority basic features: {len(basic_df[basic_df['priority'] >= 4])}\")\n",
    "\n",
    "print(\"\\n6. KEY INSIGHTS\")\n",
    "print(\"   - Season cumulative stats (plyr_rec) are highly predictive\")\n",
    "print(\"   - Game-level stats (plyr_gm_rec) show moderate-high correlation\")\n",
    "print(\"   - Rolling averages will likely improve prediction stability\")\n",
    "print(\"   - Target volatility is high - ensemble methods recommended\")\n",
    "print(\"   - Little multicollinearity among top features - good for RF models\")\n",
    "\n",
    "print(\"\\n7. RISKS & CONCERNS\")\n",
    "print(\"   - High target variance (CV > 1.0) - expect wide prediction intervals\")\n",
    "print(\"   - Right-skewed distribution - may need log transformation for some models\")\n",
    "print(\"   - Zero-yards games (7.9%) - consider classification + regression approach\")\n",
    "print(\"   - Week effects visible - may need week-specific adjustments\")\n",
    "\n",
    "print(\"\\n8. NEXT STEPS\")\n",
    "print(\"   1. Implement top 5-7 basic features for baseline model\")\n",
    "print(\"   2. Train Random Forest with current features + rolling averages\")\n",
    "print(\"   3. Evaluate baseline performance (RMSE, MAE, RÂ²)\")\n",
    "print(\"   4. Analyze feature importance from RF model\")\n",
    "print(\"   5. Iterate with complex features based on baseline results\")\n",
    "print(\"   6. Consider ensemble: RF + Gradient Boosting + Linear models\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ANALYSIS COMPLETE\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate final summary report\n",
    "summary_report = {\n",
    "    'analysis_date': '2024-11-24',\n",
    "    'dataset_file': 'nfl_wr_receiving_yards_dataset_20251124_184724.parquet',\n",
    "    'dataset_stats': {\n",
    "        'total_rows': len(df),\n",
    "        'total_columns': len(df.columns),\n",
    "        'unique_players': int(df['plyr_id'].nunique()),\n",
    "        'date_range': f\"{df['year'].min()}-{df['year'].max()}\",\n",
    "        'week_range': f\"{df['week_num'].min()}-{df['week_num'].max()}\"\n",
    "    },\n",
    "    'target_stats': {\n",
    "        'mean': float(target.mean()),\n",
    "        'median': float(target.median()),\n",
    "        'std': float(target.std()),\n",
    "        'min': float(target.min()),\n",
    "        'max': float(target.max()),\n",
    "        'skewness': float(skew(target)),\n",
    "        'kurtosis': float(kurtosis(target)),\n",
    "        'zero_yards_pct': float((target == 0).sum() / len(target) * 100)\n",
    "    },\n",
    "    'top_features': top_10[['Feature', 'Pearson_Correlation']].to_dict('records'),\n",
    "    'data_quality': {\n",
    "        'missing_values': int(df.isnull().sum().sum()),\n",
    "        'indicator_variables': len(indicator_cols)\n",
    "    },\n",
    "    'recommendations_count': {\n",
    "        'basic_features': len(basic_features),\n",
    "        'complex_features': len(complex_features)\n",
    "    }\n",
    "}\n",
    "\n",
    "import json\n",
    "with open(CSV_PATH / 'eda_summary_report.json', 'w') as f:\n",
    "    json.dump(summary_report, f, indent=2)\n",
    "\n",
    "print(f\"Summary report saved to {CSV_PATH / 'eda_summary_report.json'}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
