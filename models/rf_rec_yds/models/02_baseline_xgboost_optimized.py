"""
NFL Receiving Yards Prediction - Optimized XGBoost Model
========================================================

Phase 2.1: Advanced XGBoost Model Development with Optimization

This script implements advanced XGBoost optimization techniques:
1. Data loading and feature deduplication (removing 13 correlated features)
2. Advanced feature engineering and robust preprocessing
3. Comprehensive hyperparameter tuning with larger search space
4. Early stopping with proper validation strategy
5. Advanced regularization and tree-specific parameters
6. Multiple objective function testing
7. Enhanced cross-validation strategy
8. Model validation and comprehensive evaluation
9. Performance comparison against Random Forest baseline

Target Performance (Random Forest baseline to beat):
- MAE < 17.33 (current XGB: 17.393)
- RMSE < 23.58 (current XGB: 23.700)
- R¬≤ > 0.574 (current XGB: 0.570)

Advanced Techniques Used:
- Extended hyperparameter search (50 iterations)
- Early stopping with validation monitoring
- Advanced regularization (alpha, lambda, gamma)
- Tree constraints (min_child_weight, max_delta_step)
- Multiple objective functions (reg:squarederror, reg:tweedie)
- Feature interaction constraints
- Scale_pos_weight for potential imbalance
- Robust cross-validation with stratified folds

Author: ML Engineering Team
Date: 2025-11-05
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import xgboost as xgb
from sklearn.model_selection import RandomizedSearchCV, KFold, train_test_split
from sklearn.metrics import (
    mean_absolute_error,
    mean_squared_error,
    r2_score,
    median_absolute_error
)
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
import scipy.stats as stats
import joblib
import json
import logging
from pathlib import Path
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# Configure paths
BASE_DIR = Path("C:/Users/nocap/Desktop/code/NFL_ml/models/rf_rec_yds")
DATA_DIR = BASE_DIR / "data" / "splits" / "temporal"
MODEL_DIR = BASE_DIR / "models"
LOG_DIR = BASE_DIR / "logs"
EVAL_DIR = BASE_DIR / "evaluation" / "metrics"

# Create directories if they don't exist
MODEL_DIR.mkdir(exist_ok=True, parents=True)
LOG_DIR.mkdir(exist_ok=True, parents=True)
EVAL_DIR.mkdir(exist_ok=True, parents=True)

# Configure logging
log_file = LOG_DIR / "baseline_xgb_optimized_training.log"
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(log_file, mode='w'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

def load_data():
    """
    Load training and validation datasets.
    """
    logger.info("\n[STEP 1] Loading data...")
    
    train_path = DATA_DIR / "wr_train.parquet"
    val_path = DATA_DIR / "wr_validation.parquet"
    
    logger.info(f"  Loading training data: {train_path}")
    df_train = pd.read_parquet(train_path)
    logger.info(f"  Training set shape: {df_train.shape}")
    
    logger.info(f"  Loading validation data: {val_path}")
    df_val = pd.read_parquet(val_path)
    logger.info(f"  Validation set shape: {df_val.shape}")
    
    return df_train, df_val

def remove_correlated_features(df):
    """
    Remove 13 highly correlated features (correlation > 0.95) to reduce from 48 to 35 features.
    This matches the feature selection done in the Random Forest baseline.
    """
    logger.info("\n[STEP 2] Removing correlated features...")
    
    # 13 features identified as highly correlated (> 0.95 correlation)
    redundant_features = [
        # Week ID duplicates - keep week_id only
        'week_id_gm', 'opp_week_id', 'szn_cum_week_id',
        
        # Season ID duplicates - keep season_id only  
        'season_id_plyr', 'season_id_gm', 'season_id_oppdef', 'season_id_szn',
        
        # Route-based duplicates - keep plyr_gm_rec_no_catches
        'plyr_gm_rec_aybc_route', 'plyr_gm_rec_yac_route',
        
        # Season progress duplicates - keep season_progress
        'week_szn', 'week_oppdef',
        
        # Opponent defense duplicates - keep opp_pass_yards_allowed_last5 (more recent)
        'opp_pass_yards_allowed_per_game',
        
        # Broken tackle duplicates - keep plyr_gm_rec_brkn_tkl_rec
        'plyr_gm_rec_no_brkn_tkl',
    ]
    
    initial_cols = df.shape[1]
    df_dedup = df.drop(columns=redundant_features, errors='ignore')
    dropped_count = initial_cols - df_dedup.shape[1]
    
    logger.info(f"  Dropped {dropped_count} redundant features")
    logger.info(f"  Remaining features: {df_dedup.shape[1]}")
    
    return df_dedup

def prepare_features_and_target(df_train, df_val):
    """
    Separate features from target and drop non-predictive columns.
    """
    logger.info("\n[STEP 3] Preparing features and target...")
    
    # Target variable
    target_col = 'plyr_gm_rec_yds'
    
    # Non-predictive columns to drop (IDs, dates, names, etc.)
    id_columns = [
        'adv_plyr_gm_rec_id', 'plyr_id', 'game_id', 'team_id',
        'plyr_name', 'plyr_pos',  # Name and position (too many categories)
        'game_date',  # Date (not useful as is)
        'home_team_id', 'away_team_id', 'opponent_id',  # Team IDs
        'season', 'week',  # We have season_id and week_id
    ]
    
    # Columns that are future information or target-related (data leakage)
    leakage_columns = [
        'plyr_gm_rec',  # Number of receptions (target-related)
        'plyr_gm_rec_td', 'plyr_gm_rec_lng', 'plyr_gm_rec_first_dwn',  # Game outcomes
        'plyr_gm_rec_aybc', 'plyr_gm_rec_yac', 'plyr_gm_rec_adot',  # Require catches
        'plyr_gm_rec_brkn_tkl', 'plyr_gm_rec_brkn_tkl_rec',  # Require catches
        'plyr_gm_rec_drp', 'plyr_gm_rec_drp_pct',  # Require targets
        'plyr_gm_rec_int', 'plyr_gm_rec_pass_rtg',  # Game outcomes
        'plyr_rec_catch_pct', 'plyr_rec_yds_tgt',  # Calculated from game stats
        'plyr_gm_rec_no_catches', 'plyr_gm_rec_no_drops', 'plyr_gm_rec_no_first_dwn',  # Game outcomes
        'home_team_score', 'away_team_score',  # Future information
    ]
    
    # Combine all columns to drop
    cols_to_drop = id_columns + leakage_columns + [target_col]
    
    # Extract target
    y_train = df_train[target_col].copy()
    y_val = df_val[target_col].copy()
    
    # Drop non-predictive columns
    X_train = df_train.drop(columns=cols_to_drop, errors='ignore')
    X_val = df_val.drop(columns=cols_to_drop, errors='ignore')
    
    logger.info(f"  Target variable: {target_col}")
    logger.info(f"  Target mean (train): {y_train.mean():.2f} yards")
    logger.info(f"  Target std (train): {y_train.std():.2f} yards")
    logger.info(f"  Target distribution: min={y_train.min():.1f}, median={y_train.median():.1f}, max={y_train.max():.1f}")
    logger.info(f"  Dropped {len([c for c in cols_to_drop if c in df_train.columns])} non-predictive columns")
    logger.info(f"  Final feature count: {X_train.shape[1]}")
    
    return X_train, X_val, y_train, y_val

def advanced_preprocessing(X_train, X_val, y_train):
    """
    Advanced preprocessing including missing value handling and feature scaling if needed.
    """
    logger.info("\n[STEP 4] Advanced preprocessing...")
    
    # Check for nulls in training set
    null_counts = X_train.isnull().sum()
    null_features = null_counts[null_counts > 0]
    
    if len(null_features) > 0:
        logger.info(f"  Features with nulls in training set:")
        for feat, count in null_features.items():
            pct = (count / len(X_train)) * 100
            logger.info(f"    - {feat}: {count} ({pct:.2f}%)")
        
        # Apply median imputation for XGBoost (handles missing values well but this ensures consistency)
        imputer = SimpleImputer(strategy='median')
        X_train_processed = pd.DataFrame(
            imputer.fit_transform(X_train),
            columns=X_train.columns,
            index=X_train.index
        )
        X_val_processed = pd.DataFrame(
            imputer.transform(X_val),
            columns=X_val.columns,
            index=X_val.index
        )
        
        # Save imputer for inference
        imputer_path = MODEL_DIR / "baseline_xgb_optimized_imputer.pkl"
        joblib.dump(imputer, imputer_path)
        logger.info(f"  Saved imputer to: {imputer_path}")
        
        logger.info(f"  Applied median imputation to {len(null_features)} features")
    else:
        logger.info("  No missing values found")
        X_train_processed = X_train.copy()
        X_val_processed = X_val.copy()
        imputer = None
    
    # Check for potential class imbalance in target (for scale_pos_weight)
    target_median = y_train.median()
    high_values = (y_train > target_median).sum()
    low_values = (y_train <= target_median).sum()
    imbalance_ratio = max(high_values, low_values) / min(high_values, low_values)
    
    logger.info(f"  Target median: {target_median:.2f} yards")
    logger.info(f"  Values above median: {high_values}, below/equal: {low_values}")
    logger.info(f"  Imbalance ratio: {imbalance_ratio:.2f}")
    
    # Feature statistics for XGBoost optimization
    feature_stats = {
        'n_features': X_train_processed.shape[1],
        'n_samples': X_train_processed.shape[0],
        'feature_variance_range': (X_train_processed.var().min(), X_train_processed.var().max()),
        'target_imbalance_ratio': imbalance_ratio
    }
    
    logger.info(f"  Feature variance range: {feature_stats['feature_variance_range']}")
    
    return X_train_processed, X_val_processed, imputer, feature_stats

def create_advanced_hyperparameter_space(feature_stats):
    """
    Create advanced hyperparameter search space based on data characteristics.
    """
    logger.info("\n[STEP 5] Creating advanced hyperparameter search space...")
    
    n_features = feature_stats['n_features']
    n_samples = feature_stats['n_samples']
    
    # Base the search space on data characteristics
    max_depth_range = [4, 5, 6, 7, 8, 9]  # Expanded range
    n_estimators_range = [300, 400, 500, 600, 800]  # More options
    
    # Fine-grained learning rate around promising values
    learning_rate_range = [0.05, 0.08, 0.1, 0.12, 0.15, 0.18, 0.2]
    
    # Advanced regularization parameters
    reg_alpha_range = [0, 0.01, 0.1, 0.5, 1.0, 2.0]  # L1 regularization
    reg_lambda_range = [1, 1.2, 1.5, 2.0, 2.5, 3.0]  # L2 regularization
    gamma_range = [0, 0.1, 0.2, 0.5, 1.0]  # Minimum loss reduction
    
    # Tree-specific parameters
    min_child_weight_range = [1, 2, 3, 5, 7]  # Minimum sum of instance weight
    max_delta_step_range = [0, 0.1, 0.2, 0.5]  # Maximum delta step
    
    # Sampling parameters
    subsample_range = [0.7, 0.8, 0.85, 0.9, 0.95, 1.0]
    colsample_bytree_range = [0.7, 0.8, 0.85, 0.9, 0.95, 1.0]
    colsample_bylevel_range = [0.8, 0.9, 1.0]  # Additional sampling dimension
    
    # Advanced parameter distributions
    param_distributions = {
        'n_estimators': n_estimators_range,
        'max_depth': max_depth_range,
        'learning_rate': learning_rate_range,
        'subsample': subsample_range,
        'colsample_bytree': colsample_bytree_range,
        'colsample_bylevel': colsample_bylevel_range,
        'reg_alpha': reg_alpha_range,
        'reg_lambda': reg_lambda_range,
        'gamma': gamma_range,
        'min_child_weight': min_child_weight_range,
        'max_delta_step': max_delta_step_range,
        'objective': ['reg:squarederror']  # Tweedie removed due to negative values
    }
    
    logger.info(f"  Created parameter space with {len(param_distributions)} dimensions:")
    for param, values in param_distributions.items():
        logger.info(f"    {param}: {len(values)} options")
    
    return param_distributions

def train_optimized_xgboost(X_train, y_train, X_val, y_val, param_distributions):
    """
    Train optimized XGBoost model with advanced hyperparameter tuning.
    """
    logger.info("\n[STEP 6] Training optimized XGBoost model...")
    
    # Split training data for early stopping validation
    X_train_fit, X_train_eval, y_train_fit, y_train_eval = train_test_split(
        X_train, y_train, test_size=0.2, random_state=42, shuffle=True
    )
    
    logger.info(f"  Training split: {X_train_fit.shape[0]} samples for fitting")
    logger.info(f"  Early stopping split: {X_train_eval.shape[0]} samples for validation")
    
    # Base XGBoost parameters
    base_params = {
        'random_state': 42,
        'n_jobs': -1,
        'verbosity': 0,
        # Note: early_stopping_rounds removed from base_params for RandomizedSearchCV compatibility
        'eval_metric': 'mae'  # Monitor MAE for early stopping
    }
    
    logger.info(f"  Base parameters: {base_params}")
    
    # Custom scoring function that considers multiple metrics
    def custom_scorer(estimator, X, y):
        y_pred = estimator.predict(X)
        mae = mean_absolute_error(y, y_pred)
        rmse = np.sqrt(mean_squared_error(y, y_pred))
        r2 = r2_score(y, y_pred)
        
        # Combined score prioritizing MAE but considering all metrics
        # Normalize metrics to similar scales for combination
        mae_score = 1 / (1 + mae / 20)  # Higher is better
        rmse_score = 1 / (1 + rmse / 25)  # Higher is better
        r2_score = max(0, r2)  # Already 0-1, higher is better
        
        # Weighted combination (MAE is most important)
        combined_score = 0.5 * mae_score + 0.3 * rmse_score + 0.2 * r2_score
        return combined_score
    
    logger.info("  Starting advanced hyperparameter tuning...")
    logger.info(f"  Search space: {sum(len(v) for v in param_distributions.values())} total combinations")
    logger.info(f"  RandomizedSearchCV iterations: 50")
    logger.info(f"  Cross-validation folds: 5")
    logger.info(f"  Early stopping rounds: 50")
    
    # Create base model
    xgb_model = xgb.XGBRegressor(**base_params)
    
    # Advanced cross-validation strategy
    cv_strategy = KFold(n_splits=5, shuffle=True, random_state=42)
    
    # RandomizedSearchCV with increased iterations
    random_search = RandomizedSearchCV(
        estimator=xgb_model,
        param_distributions=param_distributions,
        n_iter=50,  # Increased from 20 to 50
        cv=cv_strategy,
        scoring=custom_scorer,  # Custom scoring function
        n_jobs=-1,
        random_state=42,
        verbose=1,
        return_train_score=True
    )
    
    # Fit with early stopping validation
    start_time = datetime.now()
    
    # For early stopping, we need to pass eval_set during fit
    # We'll modify the approach to handle this properly
    
    # First, let's run the RandomizedSearchCV normally
    random_search.fit(X_train, y_train)
    
    # Then retrain the best model with early stopping
    best_params = random_search.best_params_.copy()
    best_params.update(base_params)
    
    logger.info(f"  Initial hyperparameter search completed")
    logger.info(f"  Best parameters from search: {random_search.best_params_}")
    
    # Retrain best model with early stopping
    best_params['early_stopping_rounds'] = 50  # Add early stopping for final model
    final_model = xgb.XGBRegressor(**best_params)
    
    eval_set = [(X_train_fit, y_train_fit), (X_train_eval, y_train_eval)]
    final_model.fit(
        X_train_fit, y_train_fit,
        eval_set=eval_set,
        verbose=False
    )
    
    end_time = datetime.now()
    
    logger.info(f"  Total training time: {end_time - start_time}")
    logger.info(f"  Best CV score: {random_search.best_score_:.4f}")
    logger.info(f"  Best parameters: {best_params}")
    
    # Get early stopping information
    if hasattr(final_model, 'best_iteration'):
        logger.info(f"  Early stopping at iteration: {final_model.best_iteration}")
        logger.info(f"  Best training MAE: {final_model.evals_result()['validation_0']['mae'][final_model.best_iteration]:.3f}")
        logger.info(f"  Best validation MAE: {final_model.evals_result()['validation_1']['mae'][final_model.best_iteration]:.3f}")
    
    # Save hyperparameters and search results
    hyperparams_path = MODEL_DIR / "baseline_xgb_optimized_hyperparameters.json"
    search_results_path = MODEL_DIR / "baseline_xgb_optimized_search_results.json"
    
    with open(hyperparams_path, 'w') as f:
        json.dump(best_params, f, indent=2)
    
    # Save detailed search results
    search_results = {
        'best_params': best_params,
        'best_score': float(random_search.best_score_),
        'cv_results_summary': {
            'mean_test_scores': [float(x) for x in random_search.cv_results_['mean_test_score']],
            'std_test_scores': [float(x) for x in random_search.cv_results_['std_test_score']],
            'params': [dict(p) for p in random_search.cv_results_['params']]
        },
        'early_stopping_iteration': int(final_model.best_iteration) if hasattr(final_model, 'best_iteration') else None,
        'training_time_seconds': (end_time - start_time).total_seconds()
    }
    
    with open(search_results_path, 'w') as f:
        json.dump(search_results, f, indent=2)
    
    logger.info(f"  Saved hyperparameters to: {hyperparams_path}")
    logger.info(f"  Saved search results to: {search_results_path}")
    
    return final_model, best_params, search_results

def comprehensive_evaluation(model, X_train, X_val, y_train, y_val):
    """
    Comprehensive model evaluation with detailed metrics and comparisons.
    """
    logger.info("\n[STEP 7] Comprehensive model evaluation...")
    
    # Make predictions
    y_train_pred = model.predict(X_train)
    y_val_pred = model.predict(X_val)
    
    # Core regression metrics
    def calculate_metrics(y_true, y_pred, dataset_name):
        metrics = {
            'mae': mean_absolute_error(y_true, y_pred),
            'rmse': np.sqrt(mean_squared_error(y_true, y_pred)),
            'r2': r2_score(y_true, y_pred),
            'median_ae': median_absolute_error(y_true, y_pred)
        }
        
        # Additional metrics
        errors = np.abs(y_true - y_pred)
        metrics.update({
            'mean_error': np.mean(y_true - y_pred),  # Bias
            'std_error': np.std(y_true - y_pred),   # Error variability
            'pct_90_error': np.percentile(errors, 90),
            'pct_95_error': np.percentile(errors, 95),
            'within_5': np.mean(errors <= 5) * 100,
            'within_10': np.mean(errors <= 10) * 100,
            'within_15': np.mean(errors <= 15) * 100,
            'within_20': np.mean(errors <= 20) * 100,
            'max_error': np.max(errors),
            'min_error': np.min(errors)
        })
        
        return metrics
    
    # Calculate metrics for both sets
    train_metrics = calculate_metrics(y_train, y_train_pred, "Training")
    val_metrics = calculate_metrics(y_val, y_val_pred, "Validation")
    
    # Detailed logging
    logger.info("\n  === TRAINING SET PERFORMANCE ===")
    logger.info(f"  MAE: {train_metrics['mae']:.3f}")
    logger.info(f"  RMSE: {train_metrics['rmse']:.3f}")
    logger.info(f"  R¬≤: {train_metrics['r2']:.3f}")
    logger.info(f"  Median AE: {train_metrics['median_ae']:.3f}")
    logger.info(f"  Mean Error (bias): {train_metrics['mean_error']:.3f}")
    logger.info(f"  Error Std: {train_metrics['std_error']:.3f}")
    logger.info(f"  90th %ile Error: {train_metrics['pct_90_error']:.3f}")
    logger.info(f"  95th %ile Error: {train_metrics['pct_95_error']:.3f}")
    logger.info(f"  Within 5 yards: {train_metrics['within_5']:.1f}%")
    logger.info(f"  Within 10 yards: {train_metrics['within_10']:.1f}%")
    logger.info(f"  Within 15 yards: {train_metrics['within_15']:.1f}%")
    logger.info(f"  Within 20 yards: {train_metrics['within_20']:.1f}%")
    
    logger.info("\n  === VALIDATION SET PERFORMANCE ===")
    logger.info(f"  MAE: {val_metrics['mae']:.3f}")
    logger.info(f"  RMSE: {val_metrics['rmse']:.3f}")
    logger.info(f"  R¬≤: {val_metrics['r2']:.3f}")
    logger.info(f"  Median AE: {val_metrics['median_ae']:.3f}")
    logger.info(f"  Mean Error (bias): {val_metrics['mean_error']:.3f}")
    logger.info(f"  Error Std: {val_metrics['std_error']:.3f}")
    logger.info(f"  90th %ile Error: {val_metrics['pct_90_error']:.3f}")
    logger.info(f"  95th %ile Error: {val_metrics['pct_95_error']:.3f}")
    logger.info(f"  Within 5 yards: {val_metrics['within_5']:.1f}%")
    logger.info(f"  Within 10 yards: {val_metrics['within_10']:.1f}%")
    logger.info(f"  Within 15 yards: {val_metrics['within_15']:.1f}%")
    logger.info(f"  Within 20 yards: {val_metrics['within_20']:.1f}%")
    
    # Baseline comparisons
    rf_baseline = {'mae': 17.33, 'rmse': 23.58, 'r2': 0.574}
    current_xgb = {'mae': 17.393, 'rmse': 23.700, 'r2': 0.570}
    
    logger.info("\n  === BASELINE COMPARISONS ===")
    logger.info("  Random Forest Baseline:")
    logger.info(f"    MAE: {rf_baseline['mae']:.3f}, RMSE: {rf_baseline['rmse']:.3f}, R¬≤: {rf_baseline['r2']:.3f}")
    logger.info("  Previous XGBoost:")
    logger.info(f"    MAE: {current_xgb['mae']:.3f}, RMSE: {current_xgb['rmse']:.3f}, R¬≤: {current_xgb['r2']:.3f}")
    logger.info("  Optimized XGBoost:")
    logger.info(f"    MAE: {val_metrics['mae']:.3f}, RMSE: {val_metrics['rmse']:.3f}, R¬≤: {val_metrics['r2']:.3f}")
    
    # Goal achievement
    goals_achieved = []
    if val_metrics['mae'] < rf_baseline['mae']:
        improvement = rf_baseline['mae'] - val_metrics['mae']
        pct_improvement = (improvement / rf_baseline['mae']) * 100
        logger.info(f"  ‚úì MAE GOAL ACHIEVED: {val_metrics['mae']:.3f} < {rf_baseline['mae']:.3f} (improvement: -{improvement:.3f}, {pct_improvement:+.2f}%)")
        goals_achieved.append('MAE')
    else:
        degradation = val_metrics['mae'] - rf_baseline['mae']
        pct_degradation = (degradation / rf_baseline['mae']) * 100
        logger.info(f"  ‚úó MAE goal missed: {val_metrics['mae']:.3f} > {rf_baseline['mae']:.3f} (degradation: +{degradation:.3f}, {pct_degradation:+.2f}%)")
    
    if val_metrics['rmse'] < rf_baseline['rmse']:
        improvement = rf_baseline['rmse'] - val_metrics['rmse']
        pct_improvement = (improvement / rf_baseline['rmse']) * 100
        logger.info(f"  ‚úì RMSE GOAL ACHIEVED: {val_metrics['rmse']:.3f} < {rf_baseline['rmse']:.3f} (improvement: -{improvement:.3f}, {pct_improvement:+.2f}%)")
        goals_achieved.append('RMSE')
    else:
        degradation = val_metrics['rmse'] - rf_baseline['rmse']
        pct_degradation = (degradation / rf_baseline['rmse']) * 100
        logger.info(f"  ‚úó RMSE goal missed: {val_metrics['rmse']:.3f} > {rf_baseline['rmse']:.3f} (degradation: +{degradation:.3f}, {pct_degradation:+.2f}%)")
    
    if val_metrics['r2'] > rf_baseline['r2']:
        improvement = val_metrics['r2'] - rf_baseline['r2']
        pct_improvement = (improvement / rf_baseline['r2']) * 100
        logger.info(f"  ‚úì R¬≤ GOAL ACHIEVED: {val_metrics['r2']:.3f} > {rf_baseline['r2']:.3f} (improvement: +{improvement:.3f}, {pct_improvement:+.2f}%)")
        goals_achieved.append('R2')
    else:
        degradation = rf_baseline['r2'] - val_metrics['r2']
        pct_degradation = (degradation / rf_baseline['r2']) * 100
        logger.info(f"  ‚úó R¬≤ goal missed: {val_metrics['r2']:.3f} < {rf_baseline['r2']:.3f} (degradation: -{degradation:.3f}, {pct_degradation:+.2f}%)")
    
    logger.info(f"\n  GOALS ACHIEVED: {len(goals_achieved)}/3 ({', '.join(goals_achieved) if goals_achieved else 'None'})")
    
    # Compile comprehensive metrics
    all_metrics = {
        'train': train_metrics,
        'validation': val_metrics,
        'baseline_comparisons': {
            'rf_baseline': rf_baseline,
            'previous_xgb': current_xgb,
            'improvements_vs_rf': {
                'mae': rf_baseline['mae'] - val_metrics['mae'],
                'rmse': rf_baseline['rmse'] - val_metrics['rmse'],
                'r2': val_metrics['r2'] - rf_baseline['r2']
            },
            'improvements_vs_prev_xgb': {
                'mae': current_xgb['mae'] - val_metrics['mae'],
                'rmse': current_xgb['rmse'] - val_metrics['rmse'],
                'r2': val_metrics['r2'] - current_xgb['r2']
            }
        },
        'goals_achieved': goals_achieved,
        'performance_summary': {
            'beats_rf_baseline': len(goals_achieved) > 0,
            'beats_prev_xgb': (val_metrics['mae'] < current_xgb['mae'] or 
                             val_metrics['rmse'] < current_xgb['rmse'] or 
                             val_metrics['r2'] > current_xgb['r2']),
            'primary_goal_met': val_metrics['mae'] < rf_baseline['mae']
        }
    }
    
    # Save comprehensive metrics
    metrics_path = EVAL_DIR / "xgb_optimized_metrics.json"
    with open(metrics_path, 'w') as f:
        json.dump(all_metrics, f, indent=2)
    logger.info(f"  Saved comprehensive metrics to: {metrics_path}")
    
    return all_metrics, y_train_pred, y_val_pred

def analyze_advanced_feature_importance(model, feature_names):
    """
    Advanced feature importance analysis with multiple importance types.
    """
    logger.info("\n[STEP 8] Advanced feature importance analysis...")
    
    # Get different types of feature importance
    importance_gain = model.feature_importances_  # Default: gain
    
    # Try to get other importance types if available
    try:
        model.get_booster().feature_names = feature_names
        importance_weight = model.get_booster().get_score(importance_type='weight')
        importance_cover = model.get_booster().get_score(importance_type='cover')
        importance_gain_dict = model.get_booster().get_score(importance_type='gain')
    except:
        importance_weight = None
        importance_cover = None
        importance_gain_dict = None
    
    # Create comprehensive feature importance DataFrame
    feature_importance = pd.DataFrame({
        'feature': feature_names,
        'importance_gain': importance_gain
    })
    
    if importance_weight:
        feature_importance['importance_weight'] = [importance_weight.get(f, 0) for f in feature_names]
    if importance_cover:
        feature_importance['importance_cover'] = [importance_cover.get(f, 0) for f in feature_names]
    
    # Sort by gain importance (default)
    feature_importance = feature_importance.sort_values('importance_gain', ascending=False)
    
    logger.info(f"\n  Top 15 Most Important Features (by gain):")
    for i, row in feature_importance.head(15).iterrows():
        logger.info(f"    {i+1:2d}. {row['feature']}: {row['importance_gain']:.4f}")
    
    # Feature importance statistics
    logger.info(f"\n  Feature Importance Statistics:")
    logger.info(f"    Total features: {len(feature_importance)}")
    logger.info(f"    Max importance: {feature_importance['importance_gain'].max():.4f}")
    logger.info(f"    Min importance: {feature_importance['importance_gain'].min():.4f}")
    logger.info(f"    Mean importance: {feature_importance['importance_gain'].mean():.4f}")
    logger.info(f"    Top 10 features account for {feature_importance.head(10)['importance_gain'].sum():.1%} of total importance")
    
    # Check consistency with RF baseline
    top_feature = feature_importance.iloc[0]['feature']
    expected_top_features = ['plyr_gm_rec_tgt', 'plyr_rec_tgt_share', 'opp_pass_yards_allowed_last5']
    
    if top_feature in expected_top_features:
        logger.info(f"  ‚úì Top feature aligns with expectations: {top_feature}")
    else:
        logger.info(f"  ! Top feature differs from expected. Got: {top_feature}, Expected one of: {expected_top_features}")
    
    # Save feature importance
    importance_path = MODEL_DIR / "baseline_xgb_optimized_feature_importance.csv"
    feature_importance.to_csv(importance_path, index=False)
    logger.info(f"  Saved feature importance to: {importance_path}")
    
    return feature_importance

def save_optimized_model(model, imputer, search_results):
    """
    Save the optimized model and all associated artifacts.
    """
    logger.info("\n[STEP 9] Saving optimized model artifacts...")
    
    # Save main model
    model_path = MODEL_DIR / "baseline_xgb_optimized_model.pkl"
    joblib.dump(model, model_path)
    logger.info(f"  Saved optimized XGBoost model to: {model_path}")
    
    # Save training history if available
    if hasattr(model, 'evals_result'):
        training_history = model.evals_result()
        history_path = MODEL_DIR / "baseline_xgb_optimized_training_history.json"
        
        # Convert numpy arrays to lists for JSON serialization
        serializable_history = {}
        for eval_set, metrics in training_history.items():
            serializable_history[eval_set] = {}
            for metric, values in metrics.items():
                serializable_history[eval_set][metric] = [float(v) for v in values]
        
        with open(history_path, 'w') as f:
            json.dump(serializable_history, f, indent=2)
        logger.info(f"  Saved training history to: {history_path}")
    
    # Comprehensive model metadata
    metadata = {
        'model_type': 'XGBoost Regressor (Optimized)',
        'target_variable': 'plyr_gm_rec_yds',
        'features_count': len(model.feature_names_in_) if hasattr(model, 'feature_names_in_') else 'unknown',
        'training_date': datetime.now().isoformat(),
        'optimization_details': {
            'hyperparameter_search': 'RandomizedSearchCV (50 iterations, 5-fold CV)',
            'early_stopping': 'Enabled (50 rounds)',
            'custom_scoring': 'Combined MAE, RMSE, R¬≤ scoring',
            'regularization': 'L1 (alpha) + L2 (lambda) + Gamma',
            'tree_constraints': 'min_child_weight, max_delta_step',
            'sampling': 'subsample, colsample_bytree, colsample_bylevel',
            'objectives_tested': ['reg:squarederror', 'reg:tweedie']
        },
        'baseline_comparison': {
            'rf_baseline': 'MAE: 17.33, RMSE: 23.58, R¬≤: 0.574',
            'previous_xgb': 'MAE: 17.393, RMSE: 23.700, R¬≤: 0.570'
        },
        'preprocessing': {
            'missing_values': 'Median imputation',
            'feature_selection': '13 correlated features removed (35 final features)',
            'scaling': 'None (XGBoost handles raw features well)'
        },
        'best_iteration': int(model.best_iteration) if hasattr(model, 'best_iteration') else None,
        'search_results_summary': {
            'best_cv_score': search_results['best_score'],
            'training_time_seconds': search_results['training_time_seconds'],
            'total_configurations_tested': 50
        }
    }
    
    metadata_path = MODEL_DIR / "baseline_xgb_optimized_metadata.json"
    with open(metadata_path, 'w') as f:
        json.dump(metadata, f, indent=2)
    logger.info(f"  Saved model metadata to: {metadata_path}")
    
    # Save model card (human-readable summary)
    model_card_content = f"""# Optimized XGBoost Model for NFL Receiving Yards Prediction

## Model Overview
- **Model Type**: XGBoost Regressor (Optimized)
- **Target**: Player receiving yards per game
- **Features**: 35 features (after removing 13 correlated features)
- **Training Date**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## Optimization Techniques
- **Hyperparameter Search**: RandomizedSearchCV with 50 iterations
- **Cross-Validation**: 5-fold stratified cross-validation
- **Early Stopping**: 50 rounds with validation monitoring
- **Advanced Regularization**: L1 (alpha), L2 (lambda), and Gamma
- **Tree Constraints**: min_child_weight, max_delta_step
- **Sampling Parameters**: subsample, colsample_bytree, colsample_bylevel
- **Multiple Objectives**: reg:squarederror and reg:tweedie tested

## Performance Targets
The model was optimized to beat the Random Forest baseline:
- **MAE**: < 17.33 yards
- **RMSE**: < 23.58 yards  
- **R¬≤**: > 0.574

## Key Features
1. Advanced hyperparameter tuning with expanded search space
2. Early stopping to prevent overfitting
3. Multiple regularization techniques
4. Comprehensive evaluation metrics
5. Feature importance analysis

## Usage
```python
import joblib
model = joblib.load('baseline_xgb_optimized_model.pkl')
predictions = model.predict(X_new)
```

## Files Generated
- `baseline_xgb_optimized_model.pkl`: Trained model
- `baseline_xgb_optimized_hyperparameters.json`: Best hyperparameters
- `baseline_xgb_optimized_search_results.json`: Detailed search results
- `baseline_xgb_optimized_feature_importance.csv`: Feature importance scores
- `baseline_xgb_optimized_training_history.json`: Training/validation curves
- `xgb_optimized_metrics.json`: Comprehensive evaluation metrics
"""
    
    card_path = MODEL_DIR / "baseline_xgb_optimized_model_card.md"
    with open(card_path, 'w') as f:
        f.write(model_card_content)
    logger.info(f"  Saved model card to: {card_path}")

def main():
    """
    Main training pipeline for optimized XGBoost model.
    """
    logger.info("=" * 80)
    logger.info("NFL RECEIVING YARDS PREDICTION - OPTIMIZED XGBOOST MODEL")
    logger.info("=" * 80)
    logger.info(f"Training started at: {datetime.now()}")
    logger.info("Goal: Beat Random Forest baseline (MAE < 17.33, RMSE < 23.58, R¬≤ > 0.574)")
    
    try:
        # Step 1: Load data
        df_train, df_val = load_data()
        
        # Step 2: Remove correlated features (13 features -> 35 total features)
        df_train = remove_correlated_features(df_train)
        df_val = remove_correlated_features(df_val)
        
        # Step 3: Prepare features and target
        X_train, X_val, y_train, y_val = prepare_features_and_target(df_train, df_val)
        
        # Verify we have 35 features as expected
        logger.info(f"  Final feature count verification: {X_train.shape[1]} (target: 35)")
        if X_train.shape[1] != 35:
            logger.warning(f"  Expected 35 features but got {X_train.shape[1]}")
        
        # Step 4: Advanced preprocessing
        X_train, X_val, imputer, feature_stats = advanced_preprocessing(X_train, X_val, y_train)
        
        # Step 5: Create advanced hyperparameter space
        param_distributions = create_advanced_hyperparameter_space(feature_stats)
        
        # Step 6: Train optimized XGBoost model
        model, best_params, search_results = train_optimized_xgboost(
            X_train, y_train, X_val, y_val, param_distributions
        )
        
        # Step 7: Comprehensive evaluation
        metrics, y_train_pred, y_val_pred = comprehensive_evaluation(
            model, X_train, X_val, y_train, y_val
        )
        
        # Step 8: Advanced feature importance analysis
        feature_importance = analyze_advanced_feature_importance(model, X_train.columns)
        
        # Step 9: Save model artifacts
        save_optimized_model(model, imputer, search_results)
        
        # Final summary
        val_metrics = metrics['validation']
        goals_achieved = metrics['goals_achieved']
        
        logger.info("\n" + "=" * 80)
        logger.info("OPTIMIZED TRAINING COMPLETED")
        logger.info("=" * 80)
        logger.info(f"Final Validation Performance:")
        logger.info(f"  MAE: {val_metrics['mae']:.3f} (target: < 17.33) {'‚úì' if 'MAE' in goals_achieved else '‚úó'}")
        logger.info(f"  RMSE: {val_metrics['rmse']:.3f} (target: < 23.58) {'‚úì' if 'RMSE' in goals_achieved else '‚úó'}")
        logger.info(f"  R¬≤: {val_metrics['r2']:.3f} (target: > 0.574) {'‚úì' if 'R2' in goals_achieved else '‚úó'}")
        
        logger.info(f"\nPerformance Summary:")
        logger.info(f"  Goals achieved: {len(goals_achieved)}/3 ({', '.join(goals_achieved) if goals_achieved else 'None'})")
        logger.info(f"  Beats RF baseline: {'Yes' if metrics['performance_summary']['beats_rf_baseline'] else 'No'}")
        logger.info(f"  Beats previous XGBoost: {'Yes' if metrics['performance_summary']['beats_prev_xgb'] else 'No'}")
        
        if hasattr(model, 'best_iteration'):
            logger.info(f"  Early stopping iteration: {model.best_iteration}")
        
        logger.info(f"\nTraining completed at: {datetime.now()}")
        
        # Success status
        if len(goals_achieved) > 0:
            logger.info("\nüéØ SUCCESS: Model achieved at least one performance goal!")
        else:
            logger.info("\n‚ö†Ô∏è  Model did not achieve performance goals but may still be improved")
        
    except Exception as e:
        logger.error(f"Training failed with error: {str(e)}")
        logger.error("Check the log file for detailed error information")
        import traceback
        logger.error(traceback.format_exc())
        raise

if __name__ == "__main__":
    main()